# RAG-based MCP Server for Document Question Answering

A Retrieval-Augmented Generation (RAG) system built as an MCP (Model Context Protocol) server that enables document-based question answering. This application allows you to add PDF documents to a knowledge base and query them using semantic search.

## Features

- ğŸ“„ **PDF Document Processing**: Extract and chunk text from PDF files
- ğŸ” **Semantic Search**: Find relevant document chunks using embeddings
- ğŸ’¾ **Vector Storage**: Store and retrieve document embeddings (ChromaDB or FAISS)
- ğŸ¤– **LLM Integration**: Generate answers using HuggingFace models (Mistral, Llama, etc.)
- ğŸ”— **Full RAG Pipeline**: Retrieval + Answer Generation
- ğŸŒ **MCP Server**: Expose functionality through Model Context Protocol
- ğŸš€ **Flexible**: Works on CPU (lightweight) or GPU (Colab Pro/A100)
- â˜ï¸ **Colab Compatible**: Optimized for Google Colab with GPU support

## Architecture

### Full RAG Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PDF Files     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Document        â”‚      â”‚ Embedding Model  â”‚
â”‚ Processor       â”‚â”€â”€â”€â”€â”€â–¶â”‚ (e.g., MiniLM)   â”‚
â”‚ (Chunking)      â”‚      â”‚ Creates Vectors  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚
         â”‚                        â–¼
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚              â”‚ Vector Store     â”‚
         â”‚              â”‚ (ChromaDB/FAISS) â”‚
         â”‚              â”‚ Stores Embeddingsâ”‚
         â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â”‚              User Question
         â”‚                       â”‚
         â”‚                       â–¼
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚              â”‚ Embedding Model  â”‚
         â”‚              â”‚ (Vectorize Query)â”‚
         â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â”‚                       â–¼
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚              â”‚ Similarity Searchâ”‚
         â”‚              â”‚ Retrieve Chunks  â”‚
         â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â”‚                       â–¼
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚              â”‚ LLM (HuggingFace)â”‚
         â”‚              â”‚ Generate Answer  â”‚
         â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Final Answer
```

### Components Explained

1. **Embedding Model**: Converts text to vectors (semantic representations)
2. **Vector Store**: Database for fast similarity search of embeddings
3. **LLM**: Large Language Model that generates answers from context

## Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager

### Setup

1. **Clone or navigate to the project directory**

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up environment variables**:
   ```bash
   cp .env.example .env
   # Edit .env and configure your settings
   ```

4. **Create data directories**:
   ```bash
   mkdir -p data/documents
   mkdir -p data/chroma_db
   ```

## Configuration

Edit `.env` file to configure:

- **Embedding Model**: Choose between local models (e.g., `all-MiniLM-L6-v2`) or cloud models (requires API key)
- **Vector Store**: Select `chromadb` or `faiss`
- **Storage Paths**: Configure where to store documents and vector database
- **API Keys**: Optional - for cloud-based embeddings or LLM integration

## Usage

### Interactive Client

Run the interactive client to add documents and query them:

```bash
python client.py
```

The client provides a menu-driven interface:
1. Add PDF documents to the knowledge base
2. Query documents with questions
3. Exit

### MCP Server Mode

To run as an MCP server (for integration with MCP clients):

```bash
python -m src.mcp_server
```

### Programmatic Usage

```python
from src.rag_engine import RAGEngine

# Initialize RAG engine
rag = RAGEngine(
    embedding_model="all-MiniLM-L6-v2",
    vector_store_type="chromadb",
    storage_path="./data/chroma_db"
)

# Add a document
result = rag.add_document("path/to/document.pdf", document_id="doc1")
print(result)

# Query documents
result = rag.query("What is the main topic of this document?", top_k=5)
print(result['context'])
```

## MCP Tools

The MCP server exposes the following tools:

### `add_document`
Add a PDF document to the knowledge base.

**Parameters:**
- `pdf_path` (required): Path to the PDF file
- `document_id` (optional): Identifier for the document

### `query_documents`
Query the document knowledge base to retrieve relevant context.

**Parameters:**
- `question` (required): The question or query text
- `top_k` (optional): Number of chunks to retrieve (default: 5)

### `delete_document`
Delete a document from the knowledge base.

**Parameters:**
- `document_id` (required): Identifier of the document to delete

## Running on Google Colab

Since your laptop cannot run heavy software, you can use Google Colab Pro:

### Quick Start

1. **Open the Colab notebook**: `colab_example.ipynb`
2. **Upload project files** to Colab (or clone from GitHub)
3. **Run the setup cells** to install dependencies
4. **Initialize RAG with LLM**:
   ```python
   from src.rag_engine import RAGEngine
   
   rag = RAGEngine(
       embedding_model="all-MiniLM-L6-v2",
       vector_store_type="chromadb",
       storage_path="./data/chroma_db",
       llm_model_name="mistralai/Mistral-7B-Instruct-v0.2",  # HuggingFace model
       use_llm=True  # Enable answer generation
   )
   ```
5. **Add documents and query**:
   ```python
   # Add PDF
   rag.add_document("your_file.pdf")
   
   # Query with answer generation
   result = rag.query("Your question?", generate_answer=True)
   print(result['answer'])
   ```

### Recommended Models for Colab

- **Colab Pro/A100**: `mistralai/Mistral-7B-Instruct-v0.2` (high quality, ~14GB)
- **Colab Free**: Use retrieval-only mode (`use_llm=False`) or `distilgpt2` for testing

See `EXPLANATION.md` for detailed information about models and architecture.

## Project Structure

```
mcp_with_agentic_ai/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ document_processor.py    # PDF processing and chunking
â”‚   â”œâ”€â”€ embedding_service.py     # Embedding generation
â”‚   â”œâ”€â”€ vector_store.py          # Vector database interface
â”‚   â”œâ”€â”€ rag_engine.py            # Main RAG orchestration
â”‚   â””â”€â”€ mcp_server.py            # MCP server implementation
â”œâ”€â”€ data/                        # Data storage (created at runtime)
â”‚   â”œâ”€â”€ documents/               # PDF files storage
â”‚   â””â”€â”€ chroma_db/               # Vector database
â”œâ”€â”€ client.py                    # Interactive client application
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .env.example                 # Environment configuration template
â”œâ”€â”€ .gitignore                   # Git ignore rules
â””â”€â”€ README.md                    # This file
```

## Technologies Used

- **MCP (Model Context Protocol)**: For server interface
- **Sentence Transformers**: For local embeddings
- **ChromaDB/FAISS**: Vector databases
- **PyPDF/pdfplumber**: PDF text extraction
- **OpenAI API** (optional): For cloud-based embeddings

## Portfolio Use Case

This project demonstrates:

- âœ… **Agentic AI Systems**: RAG pipeline with autonomous document processing
- âœ… **MCP Integration**: Modern protocol-based AI agent interaction
- âœ… **Vector Databases**: Efficient semantic search implementation
- âœ… **End-to-End System**: Complete workflow from PDF ingestion to Q&A
- âœ… **Production-Ready Code**: Well-structured, modular architecture

Perfect for showcasing SLM-first agentic AI systems expertise!

## Future Enhancements

- [ ] LLM integration for answer generation (using retrieved context)
- [ ] Support for more document formats (DOCX, TXT, etc.)
- [ ] Web UI for document management and querying
- [ ] Multi-modal support (images in PDFs)
- [ ] Advanced chunking strategies (semantic chunking)
- [ ] Query expansion and refinement
- [ ] Document metadata filtering

## License

This project is part of a research portfolio focused on agentic AI systems.

## Author

Isaac Kobby - PhD Researcher in SLM-first Agentic AI Systems
Portfolio: https://isaackobby.com
